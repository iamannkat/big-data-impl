# -*- coding: utf-8 -*-
"""Exercise3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y6LNDhb7VXILW3DX5Oj14uFBiJtbhR1M

# Ερώτημα 3

Importing the Libraries
"""

import pandas as pd
import numpy as np
import csv
from scipy.sparse import csr_matrix
import sklearn.metrics as metrics
import sklearn.datasets as sk_data
from google.colab import drive
from sklearn.datasets import make_blobs
import sklearn.feature_extraction.text as sk_text
import sklearn.cluster as sk_cluster
drive.mount('/content/gdrive')

"""Importing and setting up the dataframes

Το παρακάτω πλαίσιο κώδικα είναι μια ανεπιτυχής προσπάθεια ώστε να μπορέσουμε να διαβάσουμε το μεγάλο αρχείο yelp_academic_dataset_review.json. 
Η υλοποίηση της άσκησης έχει γίνει με δυο μικρότερες εκδοχές των json αρχείων, reviews και business.
"""

from io import StringIO
#import ijson
number_of_lines = 10000
json_lines = []
user = pd.DataFrame()

f =  open('/content/gdrive/MyDrive/yelp_academic_dataset_review.json' , 'r')

while True:
    
  for i in range(number_of_lines):
        
    line = f.readline()
    json_lines.append(line)

  json_str = ''.join(json_lines)
  temp = pd.read_json(StringIO(json_str), encoding = 'utf8', lines=True) 

  if len(temp) == 0 :
    break
  #print(type(temp))
  print(temp)
  user = user.append(temp)
  json_lines.clear()

print(len(user))
#user.head()

business = pd.read_json('/content/gdrive/MyDrive/small/yelp_academic_dataset_business.json',encoding = 'utf8', lines=True) 

dataToronto_bus = business.loc[business['city'] == 'Phoenix']

dataToronto3 = dataToronto_bus.loc[dataToronto_bus['review_count'] >= 10]

selection = ['Beauty & Spas', 'Shopping' ,'Bars']
dataToronto3 = dataToronto3[pd.DataFrame(dataToronto3.categories.tolist()).isin(selection).any(1).values]

user = pd.read_json('/content/gdrive/MyDrive/small/yelp_academic_dataset_review.json',encoding = 'utf8', lines=True) 
user.head()

dataToronto3.head()

"""## Organize the data - preprocessing

Στα παρακάτω δύο κελιά κώδικα παίρνουμε για κάθε επιχείρηση όλα τα text reviews που έχουν αφήσει οι users. Έπειτα, με αυτά φτιάχνουμε το corpus, το οποίο είναι μία λίστα από strings και το κάθε string είναι το κείμενο απο reviews για την κάθε επιχείρηση.

H λίστα true_labels θα κρατήσει μία σειρά από αριθμούς [0,1,2], όπου ο κάθε αριθμός θα αναπαριστά την αντίστοιχη κατηγορία(cluster).

1.   "0" : BARS
2.   "1" : BEAUTY AND SPAS
3.   "2" : SHOPPING

Η λίστα αυτή θα χρειαστεί αργότερα για να υπολογίσουμε τον confusion matrix.
"""

filenames = []
true_labels = []

for index,row in dataToronto3.iterrows(): 

  bus_data = user.loc[user['business_id'] == row[0]]
  review_texts = pd.DataFrame(columns = ['texts'])
  review_texts['texts'] = bus_data['text']
  filename = row[7] 

  if "/" in filename:
    filename = filename.replace('/', '|')

  if "Bars" in row[4]: 
    true_labels.append(0)

  elif "Beauty & Spas" in row[4]:
    true_labels.append(1)

  elif "Shopping" in row[4]:
    true_labels.append(2)

  review_texts.to_csv(filename,index=False,header=False)
  filenames.append(filename)

true_labels = np.array(true_labels)

print(true_labels)

"""Υλοποίηση του corpus, το οποίο ειναι μια λίστα όπου το κάθε στοιχείο είναι όλα τα text reviews μίας επιχείρησης."""

corpus = []
temp_doc = []
for fil in filenames:
  with open(fil, 'r') as csv_file:
    csv_reader = csv.reader(csv_file)

    for line in csv_reader: 
      
      temp_str = ''.join(line)
      temp_doc.append(temp_str)

  doc_string = ''.join(temp_doc)
  corpus.append(doc_string)
  temp_doc.clear()

print(len(corpus))

"""tf-idf αναπαράσταση των επιχειρήσεων"""

vectorizer = sk_text.TfidfVectorizer(stop_words = 'english', min_df=10)
X = vectorizer.fit_transform(corpus)
print(X.toarray())
print (vectorizer.get_feature_names())

"""# 1.

k-means
"""

kmeans = sk_cluster.KMeans(init='k-means++', n_clusters=3, n_init=10)
kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_
kmeans_labels = kmeans.labels_
error = kmeans.inertia_
print ("The total error of the clustering is: ", error)
print ('\nCluster labels')
print(kmeans_labels)
print ('\n Cluster Centroids')
print (centroids)

"""Οι πιο συχνοι όροι ανα cluster"""

k=3
print("Top terms per cluster:")
asc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]
order_centroids = asc_order_centroids[:,::-1]
terms = vectorizer.get_feature_names()
for i in range(k):
  print ("Cluster %d:" % i)
  for ind in order_centroids[i, :10]:
    print (' %s' % terms[ind])
  print

C1= metrics.confusion_matrix(kmeans_labels,true_labels)
print (C1)

plt.pcolormesh(C1,cmap=plt.cm.Reds)

p = metrics.precision_score(true_labels,kmeans_labels, average=None)
print(p)
r = metrics.recall_score(true_labels,kmeans_labels, average = None)
print(r)

def cluster_class_mapping(kmeans_labels,true_labels):
  C= metrics.confusion_matrix(kmeans_labels,true_labels)
  mapping = list(np.argmax(C,axis=1)) #for each row (cluster) find the best␣,→class in the confusion matrix
  mapped_kmeans_labels = [mapping[l] for l in kmeans_labels]
  C2= metrics.confusion_matrix(mapped_kmeans_labels,true_labels)
  return mapped_kmeans_labels,C2

mapped_kmeans_labels,C = cluster_class_mapping(kmeans_labels, true_labels)
print(C)

agglo = sk_cluster.AgglomerativeClustering(linkage = 'complete', n_clusters = 3)
agglo_labels = agglo.fit_predict(X.toarray())
C_agglo= metrics.confusion_matrix(agglo_labels,true_labels)
print (C_agglo)


mapped_agglo_labels,C_agglo = cluster_class_mapping(agglo_labels,true_labels)
print(C_agglo)
p = metrics.precision_score(true_labels,mapped_agglo_labels, average='weighted')
print(p)
r = metrics.recall_score(true_labels,mapped_agglo_labels, average = 'weighted')
print(r)

agglo = sk_cluster.AgglomerativeClustering(linkage = 'average', n_clusters = 3)
agglo_labels = agglo.fit_predict(X.toarray())
C_agglo= metrics.confusion_matrix(agglo_labels,true_labels)
print (C_agglo)


mapped_agglo_labels,C_agglo = cluster_class_mapping(agglo_labels,true_labels)
print(C_agglo)
p = metrics.precision_score(true_labels,mapped_agglo_labels, average='weighted')
print(p)
r = metrics.recall_score(true_labels,mapped_agglo_labels, average = 'weighted')
print(r)

agglo = sk_cluster.AgglomerativeClustering(linkage = 'single', n_clusters = 3)
agglo_labels = agglo.fit_predict(X.toarray())
C_agglo= metrics.confusion_matrix(agglo_labels,true_labels)
print (C_agglo)


mapped_agglo_labels,C_agglo = cluster_class_mapping(agglo_labels,true_labels)
print(C_agglo)
p = metrics.precision_score(true_labels,mapped_agglo_labels, average='weighted')
print(p)
r = metrics.recall_score(true_labels,mapped_agglo_labels, average = 'weighted')
print(r)

agglo = sk_cluster.AgglomerativeClustering(linkage = 'ward', n_clusters = 3)
agglo_labels = agglo.fit_predict(X.toarray())
C_agglo= metrics.confusion_matrix(agglo_labels,true_labels)
print (C_agglo)


mapped_agglo_labels,C_agglo = cluster_class_mapping(agglo_labels,true_labels)
print(C_agglo)
p = metrics.precision_score(true_labels,mapped_agglo_labels, average='weighted')
print(p)
r = metrics.recall_score(true_labels,mapped_agglo_labels, average = 'weighted')
print(r)

"""# 2.

Sihlouette score and plot
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt

error = np.zeros(11) # change
sh_score = np.zeros(11) # change

for k in range(1,11):
  kmeans = sk_cluster.KMeans(init='k-means++', n_clusters=k, n_init=10)
  kmeans.fit_predict(X)
  error[k] = kmeans.inertia_
  if k>1: sh_score[k]= metrics.silhouette_score(X, kmeans.labels_)

plt.plot(range(1,len(error)),error[1:])
plt.xlabel('Number of clusters')
plt.ylabel('Error')

plt.plot(range(2,len(sh_score)),sh_score[2:])
plt.xlabel('Number of clusters')
plt.ylabel('silhouette score')

"""Απο τις κορυφώσεις του διαγράματος φαίνεται πως οι προτιμότερες τιμές του k ειναι το 3 και το 9,
Επομένως, παρακάτω θα τρεξουμε ξανά τον k-means αλγόριθμο και για k=9 (n_clusters)
"""

kmeans = sk_cluster.KMeans(init='k-means++', n_clusters=9, n_init=10)
kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_
kmeans_labels = kmeans.labels_
error = kmeans.inertia_
print ("The total error of the clustering is: ", error)
print ('\nCluster labels')
print(kmeans_labels)
print ('\n Cluster Centroids')
print (centroids)

"""Παρατηρούμε πως για k=9 το total error of the clustering είναι 250.95784728801823, το οποίο είναι μικρότερο από εκείνο για k=3: 274.6735793674228. 
Φαίνεται πως ο αλγόριθμος κάνει καλύτερες προβλέψεις για μεγαλύτερο αριθμό clusters, παρόλλο που υπάρχει πιθανότητα για "overfitting"


"""

C= metrics.confusion_matrix(kmeans_labels, true_labels)
print (C)

plt.pcolormesh(C,cmap=plt.cm.Reds)